# -*- coding: utf-8 -*-
"""kaggle_animal_shelter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17i7oOfhOWcPNU59JMOjsn1c7hGtoYA_O
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

train = pd.read_csv('/content/train.csv')
test = pd.read_csv('/content/test.csv')
full = pd.concat([train, test], ignore_index=True)
full.head()

full.shape

full.columns

full.isna().sum()

from collections import Counter
Counter(train['OutcomeType'])

full = full.drop(columns = ['OutcomeSubtype', 'AnimalID', 'Name', 'DateTime', 'Color'])
Y = full['OutcomeType']

full.head()

full.drop(full.columns[len(full.columns)-1], axis=1, inplace=True)
full.head()

full.isna().sum()

full = full[full['SexuponOutcome'].notna()]
full = full[full['AgeuponOutcome'].notna()]
full.isna().sum()

# full.AgeuponOutcome.str.extract('(\d+)')
for i in full.index:
  string = str(full.at[i, 'AgeuponOutcome'])
  string_list = string.split(' ')
  num, text = string_list
  if text=='months' or text=='month':
    num = int(num) * 30
  elif text=='weeks' or text=='week':
    num = int(num) * 7
  elif text=='years' or text=='year':
    num = int(num) * 365
  elif text=='days' or text=='day':
    num = int(num)
      
  full.at[i, 'AgeuponOutcome'] = num

full.head()

categorical_columns = ['AnimalType', 'SexuponOutcome', 'Breed']
numerical_columns = ['AgeuponOutcome']
outputs = ['OutcomeType']

full.dtypes

for category in categorical_columns:
    full[category] = full[category].astype('category')

full.dtypes

for num_col in numerical_columns:
    full[num_col] = full[num_col].astype('int64')

full.dtypes

train_X = full[:26710]
test_X = full[26710:]
del test_X['OutcomeType']
print(train_X.shape)
print(test_X.shape)

test_X.head()

from sklearn.preprocessing import LabelEncoder
print(Counter(train_X['OutcomeType']))
train_X['OutcomeType'] = LabelEncoder().fit_transform(train_X['OutcomeType'])
print(Counter(train_X['OutcomeType']))

target_dict = {
    'Return_to_owner' : 3,
    'Euthanasia': 2,
    'Adoption': 0,
    'Transfer': 4,
    'Died': 1
}

train_X.head()

train_X.dtypes

"""Embedding train data"""

atype = train_X['AnimalType'].cat.codes.values
sex = train_X['SexuponOutcome'].cat.codes.values
brd = train_X['Breed'].cat.codes.values

categorical_data = np.stack([atype, sex, brd], 1)

categorical_data[:10]

categorical_data = torch.tensor(categorical_data, dtype=torch.int64)
categorical_data[:10]

numerical_data = np.stack([train_X[col].values for col in numerical_columns], 1)
numerical_data = torch.tensor(numerical_data, dtype=torch.float)
numerical_data[:5]

outputs = torch.tensor(train_X[outputs].values).flatten()
outputs[:5]

print(categorical_data.shape)
print(numerical_data.shape)
print(outputs.shape)

categorical_column_sizes = [len(train_X[column].cat.categories) for column in categorical_columns]
categorical_embedding_sizes = [(col_size, min(50, (col_size+1)//2)) for col_size in categorical_column_sizes]
print(categorical_embedding_sizes)

total_records = 26710
test_records = int(total_records * .2)

categorical_train_data = categorical_data[:total_records-test_records]
categorical_test_data = categorical_data[total_records-test_records:total_records]
numerical_train_data = numerical_data[:total_records-test_records]
numerical_test_data = numerical_data[total_records-test_records:total_records]
train_outputs = outputs[:total_records-test_records]
test_outputs = outputs[total_records-test_records:total_records]

print(len(categorical_train_data))
print(len(numerical_train_data))
print(len(train_outputs))

print(len(categorical_test_data))
print(len(numerical_test_data))
print(len(test_outputs))

class Model(nn.Module):

    def __init__(self, embedding_size, num_numerical_cols, output_size, layers, p=0.4):
        super().__init__()
        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])
        self.embedding_dropout = nn.Dropout(p)
        self.batch_norm_num = nn.BatchNorm1d(num_numerical_cols)

        all_layers = []
        num_categorical_cols = sum((nf for ni, nf in embedding_size))
        input_size = num_categorical_cols + num_numerical_cols

        for i in layers:
            all_layers.append(nn.Linear(input_size, i))
            all_layers.append(nn.ReLU(inplace=True))
            all_layers.append(nn.BatchNorm1d(i))
            all_layers.append(nn.Dropout(p))
            input_size = i

        all_layers.append(nn.Linear(layers[-1], output_size))

        self.layers = nn.Sequential(*all_layers)

    def forward(self, x_categorical, x_numerical):
        embeddings = []
        for i,e in enumerate(self.all_embeddings):
            embeddings.append(e(x_categorical[:,i]))
        x = torch.cat(embeddings, 1)
        x = self.embedding_dropout(x)

        x_numerical = self.batch_norm_num(x_numerical)
        x = torch.cat([x, x_numerical], 1)
        x = self.layers(x)
        return x

"""Training the model"""

model = Model(categorical_embedding_sizes, numerical_data.shape[1], 5, [150,100,50], p=0.1)

print(model)

loss_function = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

epochs = 400
aggregated_losses = []

for i in range(epochs):
    i += 1
    y_pred = model(categorical_train_data, numerical_train_data)
    single_loss = loss_function(y_pred, train_outputs)
    aggregated_losses.append(single_loss)

    if i%25 == 1:
        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')

    optimizer.zero_grad()
    single_loss.backward()
    optimizer.step()

print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')

plt.plot(range(epochs), aggregated_losses)
plt.ylabel('Loss')
plt.xlabel('epoch');

"""Make predictions"""

with torch.no_grad():
    y_val = model(categorical_test_data, numerical_test_data)
    loss = loss_function(y_val, test_outputs)
print(f'Loss: {loss:.8f}')

print(y_val[:5])

y_val = np.argmax(y_val, axis=1)

print(y_val[:5])

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(confusion_matrix(test_outputs,y_val))
print(classification_report(test_outputs,y_val))
print(accuracy_score(test_outputs, y_val))

